# -*- coding: utf-8 -*-
"""
Client MCP HTTP + LLMaaS - Exemple P√©dagogique
===============================================

Ce script d√©montre comment se connecter √† un serveur MCP via HTTP/SSE
et utiliser ses outils avec l'API LLMaaS en utilisant le client standard MCP.

Architecture HTTP/SSE :
-----------------------
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CE SCRIPT (mcp_client_demo.py)                 ‚îÇ
‚îÇ  R√¥le : Client et Orchestrateur                 ‚îÇ
‚îÇ  ‚Ä¢ Utilise mcp.client.sse.sse_client            ‚îÇ
‚îÇ  ‚Ä¢ Se connecte au endpoint /sse                 ‚îÇ
‚îÇ  ‚Ä¢ G√®re la session automatiquement              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ÜïÔ∏è HTTP/SSE
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Serveur MCP         ‚îÇ    ‚îÇ  API LLMaaS          ‚îÇ
‚îÇ  (mcp_server.py)     ‚îÇ    ‚îÇ  (Cloud Temple)      ‚îÇ
‚îÇ                      ‚îÇ    ‚îÇ                      ‚îÇ
‚îÇ  http://localhost    ‚îÇ    ‚îÇ  Mod√®le :            ‚îÇ
‚îÇ  :8000               ‚îÇ    ‚îÇ  qwen3-next:80b      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

import os
import json
import argparse
import httpx
import asyncio
from dotenv import load_dotenv

# Import du client SSE standard de la librairie MCP
from mcp.client.sse import sse_client
from mcp import ClientSession

# ============================================================================
# SECTION 1 : Configuration
# ============================================================================

load_dotenv()

API_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
API_KEY = os.getenv("API_KEY")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://localhost:8000")


# ============================================================================
# SECTION 2 : Conversion des Outils MCP vers Format OpenAI
# ============================================================================

def convert_mcp_tools_to_openai(list_tools_result) -> list:
    """
    Convertit les outils du format MCP vers le format attendu par l'API OpenAI/LLMaaS.
    """
    openai_tools = []
    
    # list_tools_result est un objet ListToolsResult qui contient une liste 'tools'
    for mcp_tool in list_tools_result.tools:
        openai_tool = {
            "type": "function",
            "function": {
                "name": mcp_tool.name,
                "description": mcp_tool.description,
                "parameters": mcp_tool.inputSchema
            }
        }
        openai_tools.append(openai_tool)
    
    return openai_tools


# ============================================================================
# SECTION 3 : Logique Principale Asynchrone
# ============================================================================

async def run_mcp_demo(args):
    """
    Fonction principale asynchrone qui ex√©cute la d√©monstration compl√®te.
    """
    
    # V√©rifications pr√©liminaires
    if not API_KEY:
        print("‚ùå Erreur: La variable d'environnement API_KEY n'est pas d√©finie.")
        return
    
    model_to_use = args.model if args.model else os.getenv("DEFAULT_MODEL", "qwen3-next:80b")
    
    print("=" * 70)
    print("ü§ñ D√âMONSTRATION MCP HTTP + LLMaaS")
    print("=" * 70)
    print(f"ü§ñ Mod√®le utilis√© : {model_to_use}")
    print(f"üåê Serveur MCP    : {MCP_SERVER_URL}")
    print(f"‚ö° Mode streaming : {'Activ√©' if args.stream else 'D√©sactiv√©'}")
    print("=" * 70)
    
    # URL du endpoint SSE (par d√©faut /sse avec FastMCP)
    sse_url = f"{MCP_SERVER_URL}/sse"
    
    # R√©cup√©ration de la cl√© d'auth serveur (optionnelle)
    server_auth_key = os.getenv("MCP_SERVER_AUTH_KEY")
    headers = {}
    if server_auth_key:
        headers["Authorization"] = f"Bearer {server_auth_key}"
        print(f"üîí Authentification activ√©e pour le serveur MCP.")
    
    print(f"\nüîå Connexion au endpoint SSE : {sse_url}")
    
    try:
        # Utilisation du client context manager 'sse_client' fourni par mcp
        # On passe les headers pour l'authentification
        async with sse_client(sse_url, headers=headers) as (read_stream, write_stream):
            print("‚úÖ Connexion SSE √©tablie.")
            
            # Cr√©ation de la session MCP sur les flux de lecture/√©criture
            async with ClientSession(read_stream, write_stream) as session:
                print("‚úÖ Session MCP initialis√©e.")
                
                # √âTAPE 1 : Initialisation et liste des outils
                await session.initialize()
                
                print("\nüìã R√©cup√©ration de la liste des outils...")
                result = await session.list_tools()
                
                if not result.tools:
                    print("‚ùå Aucun outil disponible sur le serveur MCP.")
                    return
                
                for tool in result.tools:
                    print(f"   ‚Ä¢ {tool.name}: {tool.description}")
                
                # Conversion pour LLMaaS
                openai_tools = convert_mcp_tools_to_openai(result)
                
                # √âTAPE 2 : Appel au LLM
                print("\n" + "‚îÄ" * 70)
                print("√âTAPE 2 : Envoi de la question au LLM")
                print("‚îÄ" * 70)
                
                user_question = "Bonjour, peux-tu me dire quelle heure il est actuellement ?"
                print(f"üí¨ Question : \"{user_question}\"")
                
                messages = [{"role": "user", "content": user_question}]
                
                payload = {
                    "model": model_to_use,
                    "messages": messages,
                    "tools": openai_tools,
                    "tool_choice": "auto",
                    "stream": args.stream
                }
                
                # Appel API LLMaaS
                # On utilise un bloc try/except sp√©cifique ici car si une erreur survient
                # en dehors du bloc 'async with ClientSession', elle sera mieux g√©r√©e
                # qu'une ExceptionGroup issue de la session.
                try:
                    async with httpx.AsyncClient() as client:
                        if args.stream:
                            # Gestion du streaming (simplifi√©e pour la d√©mo)
                            async with client.stream(
                                "POST",
                                f"{API_URL}/chat/completions",
                                headers={"Authorization": f"Bearer {API_KEY}"},
                                json=payload,
                                timeout=60
                            ) as response:
                                response.raise_for_status()
                                
                                assistant_message = {"role": "assistant", "content": None, "tool_calls": []}
                                
                                async for chunk in response.aiter_bytes():
                                    # Pour simplifier cette d√©mo, on n'affiche pas tout le parsing stream complexe
                                    # mais on assume que le mod√®le va demander un outil rapidement.
                                    pass 
                                
                                # Note: Pour une vraie impl√©mentation streaming robuste, voir les exemples pr√©c√©dents.
                                pass
                        
                        # Pour assurer le succ√®s de la d√©mo MCP, utilisons le mode non-streaming pour la logique d'appel
                        response = await client.post(
                            f"{API_URL}/chat/completions",
                            headers={"Authorization": f"Bearer {API_KEY}"},
                            json=payload,
                            timeout=60
                        )
                        response.raise_for_status()
                        response_data = response.json()
                    
                    assistant_message = response_data["choices"][0]["message"]
                    messages.append(assistant_message)
                except Exception as llm_error:
                    print(f"‚ùå Erreur lors de l'appel LLM : {llm_error}")
                    return

                # √âTAPE 3 : Ex√©cution de l'outil via MCP
                if assistant_message.get("tool_calls"):
                    tool_call = assistant_message["tool_calls"][0]
                    function_name = tool_call["function"]["name"]
                    arguments_str = tool_call["function"]["arguments"]
                    tool_call_id = tool_call["id"]
                    
                    print("\n" + "‚îÄ" * 70)
                    print("√âTAPE 3 : Ex√©cution de l'outil via le serveur MCP")
                    print("‚îÄ" * 70)
                    print(f"‚úÖ Le LLM a demand√© d'utiliser l'outil : {function_name}")
                    
                    try:
                        arguments = json.loads(arguments_str) if arguments_str else {}
                    except json.JSONDecodeError:
                        arguments = {}
                    
                    # Appel de l'outil via la session MCP standard
                    print(f"üîß Appel de l'outil '{function_name}' via session MCP...")
                    tool_result = await session.call_tool(function_name, arguments)
                    
                    # Le r√©sultat peut √™tre une liste de contenus (TextContent, ImageContent)
                    # On extrait le texte du premier contenu s'il est de type texte
                    result_text = ""
                    if tool_result.content:
                        first_content = tool_result.content[0]
                        # Utilisation de getattr pour √©viter les erreurs de typage statique
                        result_text = getattr(first_content, 'text', str(first_content))
                    
                    print(f"‚úÖ R√©sultat de l'outil : {result_text}")
                    
                    # √âTAPE 4 : R√©ponse finale
                    print("\n" + "‚îÄ" * 70)
                    print("√âTAPE 4 : Envoi du r√©sultat au LLM pour la r√©ponse finale")
                    print("‚îÄ" * 70)
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": result_text
                    })
                    
                    payload_final = {
                        "model": model_to_use,
                        "messages": messages,
                        "stream": args.stream
                    }
                    
                    async with httpx.AsyncClient() as client:
                        if args.stream:
                            async with client.stream(
                                "POST",
                                f"{API_URL}/chat/completions",
                                headers={"Authorization": f"Bearer {API_KEY}"},
                                json=payload_final,
                                timeout=60
                            ) as response_final:
                                response_final.raise_for_status()
                                async for chunk in response_final.aiter_bytes():
                                    try:
                                        decoded_chunk = chunk.decode("utf-8")
                                        for line in decoded_chunk.splitlines():
                                            if line.startswith("data: "):
                                                json_data = line[len("data: "):]
                                                if json_data.strip() == "[DONE]": continue
                                                delta = json.loads(json_data)["choices"][0]["delta"]
                                                if "content" in delta:
                                                    print(delta["content"], end="", flush=True)
                                    except: pass
                                print()
                        else:
                            response_final = await client.post(
                                f"{API_URL}/chat/completions",
                                headers={"Authorization": f"Bearer {API_KEY}"},
                                json=payload_final,
                                timeout=60
                            )
                            print(f"\nüí¨ {response_final.json()['choices'][0]['message']['content']}")
                else:
                    print("ü§î Le mod√®le n'a pas demand√© d'outil.")
                    print(assistant_message.get("content"))

    except Exception as e:
        print(f"\n‚ùå Erreur : {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--stream", action="store_true")
    parser.add_argument("--model", type=str)
    parser.add_argument("--debug", action="store_true")
    args = parser.parse_args()
    
    asyncio.run(run_mcp_demo(args))
