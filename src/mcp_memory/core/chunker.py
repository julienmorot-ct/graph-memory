# -*- coding: utf-8 -*-
"""
SemanticChunker - D√©coupage s√©mantique des documents.

Strat√©gie de chunking respectant les fronti√®res naturelles du texte :
1. D√©tecte la structure : articles num√©rot√©s, headers Markdown, titres en majuscules
2. D√©coupe par sections s√©mantiques (articles, paragraphes)
3. Ne coupe JAMAIS au milieu d'une phrase
4. Regroupe les petits blocs, sous-d√©coupe les gros
5. Overlap intelligent entre chunks adjacents (contexte aux fronti√®res)
6. Chaque chunk porte ses m√©tadonn√©es (section, article, hi√©rarchie)

Best practices impl√©ment√©es :
- Recursive splitting : s√©parateurs du plus structurel au plus fin
- Sentence-level granularity : la phrase est l'unit√© atomique
- Context preservation : titre de section/article en pr√©fixe
- Overlap at sentence boundaries : l'overlap ne coupe pas les phrases
"""

import re
import sys
from typing import Optional, List, Tuple
from dataclasses import dataclass, field

from ..config import get_settings
from .models import Chunk


# =============================================================================
# Patterns de d√©tection de structure
# =============================================================================

# Articles num√©rot√©s (juridique) : "Article 1", "Article 23.2", "ARTICLE 1er"
ARTICLE_PATTERN = re.compile(
    r'^(?:ARTICLE|Article|article)\s+(\d+(?:\.\d+)*(?:\s*(?:er|√®me|eme))?)\s*[:\.\s‚Äì‚Äî-]',
    re.MULTILINE
)

# Num√©rotation hi√©rarchique : "1.", "1.1", "1.1.1", "23.2 ‚Äì"
NUMBERED_SECTION_PATTERN = re.compile(
    r'^(\d+(?:\.\d+)+)\s*[:\.\s‚Äì‚Äî-]',
    re.MULTILINE
)

# Headers Markdown : "## Titre", "### Sous-titre"
MARKDOWN_HEADER_PATTERN = re.compile(
    r'^(#{1,6})\s+(.+)$',
    re.MULTILINE
)

# Titres en majuscules (minimum 4 mots, pas juste un acronyme)
UPPERCASE_TITLE_PATTERN = re.compile(
    r'^([A-Z√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ô√õ√ú≈∏√á][A-Z√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ô√õ√ú≈∏√á\s,\'-]{15,})$',
    re.MULTILINE
)

# S√©parateurs de phrases (pour le split au niveau phrase)
SENTENCE_ENDINGS = re.compile(
    r'(?<=[.!?])\s+(?=[A-Z√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ô√õ√ú≈∏])'
)


@dataclass
class TextSection:
    """Section d√©tect√©e dans le document."""
    title: str
    content: str
    level: int = 0  # 0 = article/section principale, 1 = sous-section, etc.
    article_number: Optional[str] = None
    start_pos: int = 0


@dataclass 
class SentenceGroup:
    """Groupe de phrases formant un chunk potentiel."""
    sentences: List[str] = field(default_factory=list)
    section_title: Optional[str] = None
    article_number: Optional[str] = None
    heading_hierarchy: List[str] = field(default_factory=list)
    
    @property
    def text(self) -> str:
        return " ".join(self.sentences)
    
    @property
    def token_estimate(self) -> int:
        """Estimation grossi√®re : ~1 token = ~4 caract√®res en fran√ßais."""
        return len(self.text) // 4


class SemanticChunker:
    """
    Chunker s√©mantique respectant les fronti√®res naturelles du texte.
    
    Strat√©gie en 3 passes :
    1. DETECT : Identifier les sections structurelles (articles, titres, headers)
    2. SPLIT  : D√©couper chaque section en phrases
    3. MERGE  : Regrouper les phrases en chunks de taille cible avec overlap
    """
    
    def __init__(self):
        """Initialise le chunker avec les param√®tres de configuration."""
        settings = get_settings()
        self._chunk_size = settings.chunk_size  # tokens
        self._chunk_overlap = settings.chunk_overlap  # tokens
    
    def chunk_document(self, text: str, filename: str) -> List[Chunk]:
        """
        D√©coupe un document en chunks s√©mantiques.
        
        Args:
            text: Contenu textuel complet du document
            filename: Nom du fichier (pour les m√©tadonn√©es)
            
        Returns:
            Liste de Chunk avec m√©tadonn√©es s√©mantiques
        """
        if not text or not text.strip():
            return []
        
        # Normaliser les fins de ligne
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # === PASSE 1 : D√©tecter la structure ===
        sections = self._detect_sections(text)
        
        total_chars = sum(len(s.content) for s in sections)
        print(f"üìê [Chunker] PASSE 1/3 ‚Äî {len(sections)} sections d√©tect√©es dans '{filename}' ({total_chars} chars)", file=sys.stderr)
        sys.stderr.flush()
        for i, s in enumerate(sections):
            art = f" (Art. {s.article_number})" if s.article_number else ""
            print(f"   üìÑ [{i+1}/{len(sections)}] {s.title[:70]}{art} ‚Äî {len(s.content)} chars, level={s.level}", file=sys.stderr)
        sys.stderr.flush()
        
        # === PASSE 2 : D√©couper chaque section en phrases ===
        print(f"üìê [Chunker] PASSE 2/3 ‚Äî D√©coupage en phrases...", file=sys.stderr)
        sys.stderr.flush()
        sentence_groups = self._sections_to_sentence_groups(sections)
        total_sentences = sum(len(g.sentences) for g in sentence_groups)
        print(f"üìê [Chunker] PASSE 2/3 ‚Äî {total_sentences} phrases dans {len(sentence_groups)} groupes", file=sys.stderr)
        sys.stderr.flush()
        
        # === PASSE 3 : Regrouper les phrases en chunks avec overlap ===
        print(f"üìê [Chunker] PASSE 3/3 ‚Äî Fusion en chunks (cible: {self._chunk_size} tokens, overlap: {self._chunk_overlap})...", file=sys.stderr)
        sys.stderr.flush()
        raw_chunks = self._merge_into_chunks(sentence_groups)
        print(f"üìê [Chunker] PASSE 3/3 ‚Äî {len(raw_chunks)} chunks bruts g√©n√©r√©s", file=sys.stderr)
        sys.stderr.flush()
        
        # === Finaliser les Chunk avec m√©tadonn√©es ===
        total = len(raw_chunks)
        chunks = []
        for i, (group, chunk_text) in enumerate(raw_chunks):
            chunk = Chunk(
                text=chunk_text.strip(),
                index=i,
                total_chunks=total,
                filename=filename,
                section_title=group.section_title,
                article_number=group.article_number,
                heading_hierarchy=group.heading_hierarchy,
                char_count=len(chunk_text.strip()),
                token_estimate=len(chunk_text.strip()) // 4
            )
            chunks.append(chunk)
        
        print(f"‚úÖ [Chunker] '{filename}' ‚Üí {len(chunks)} chunks "
              f"(cible: {self._chunk_size} tokens, overlap: {self._chunk_overlap})", file=sys.stderr)
        
        return chunks
    
    # =========================================================================
    # PASSE 1 : D√©tection de la structure
    # =========================================================================
    
    def _detect_sections(self, text: str) -> List[TextSection]:
        """
        D√©tecte les sections structurelles du document.
        
        Ordre de priorit√© :
        1. Articles num√©rot√©s (juridique)
        2. Headers Markdown
        3. Num√©rotation hi√©rarchique (1.1, 1.1.1)
        4. Titres en majuscules
        5. Double saut de ligne (paragraphes)
        
        Si aucune structure d√©tect√©e, retourne le texte entier comme section unique.
        """
        # Essayer de d√©tecter des articles num√©rot√©s (documents juridiques)
        sections = self._detect_articles(text)
        if sections and len(sections) > 1:
            return sections
        
        # Essayer les headers Markdown
        sections = self._detect_markdown_headers(text)
        if sections and len(sections) > 1:
            return sections
        
        # Essayer la num√©rotation hi√©rarchique
        sections = self._detect_numbered_sections(text)
        if sections and len(sections) > 1:
            return sections
        
        # Essayer les titres en majuscules
        sections = self._detect_uppercase_titles(text)
        if sections and len(sections) > 1:
            return sections
        
        # Fallback : d√©couper par double saut de ligne (paragraphes)
        sections = self._detect_paragraphs(text)
        return sections
    
    def _detect_articles(self, text: str) -> List[TextSection]:
        """D√©tecte les articles num√©rot√©s (documents juridiques)."""
        matches = list(ARTICLE_PATTERN.finditer(text))
        if not matches:
            return []
        
        sections = []
        
        # Texte avant le premier article (pr√©ambule)
        if matches[0].start() > 0:
            preamble = text[:matches[0].start()].strip()
            if preamble:
                sections.append(TextSection(
                    title="Pr√©ambule",
                    content=preamble,
                    level=0,
                    start_pos=0
                ))
        
        # Chaque article
        for i, match in enumerate(matches):
            article_num = match.group(1).strip()
            start = match.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            
            # Le titre de l'article = la premi√®re ligne
            first_line_end = text.find('\n', start)
            if first_line_end == -1:
                first_line_end = end
            title = text[start:first_line_end].strip()
            
            content = text[start:end].strip()
            
            sections.append(TextSection(
                title=title,
                content=content,
                level=0,
                article_number=article_num,
                start_pos=start
            ))
        
        return sections
    
    def _detect_markdown_headers(self, text: str) -> List[TextSection]:
        """D√©tecte les headers Markdown (## Titre)."""
        matches = list(MARKDOWN_HEADER_PATTERN.finditer(text))
        if not matches:
            return []
        
        sections = []
        
        # Texte avant le premier header
        if matches[0].start() > 0:
            preamble = text[:matches[0].start()].strip()
            if preamble:
                sections.append(TextSection(
                    title="Introduction",
                    content=preamble,
                    level=0,
                    start_pos=0
                ))
        
        for i, match in enumerate(matches):
            level = len(match.group(1))  # Nombre de #
            title = match.group(2).strip()
            start = match.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            content = text[start:end].strip()
            
            sections.append(TextSection(
                title=title,
                content=content,
                level=level - 1,  # ## = level 1, ### = level 2
                start_pos=start
            ))
        
        return sections
    
    def _detect_numbered_sections(self, text: str) -> List[TextSection]:
        """D√©tecte les sections num√©rot√©es (1.1, 1.1.1, etc.)."""
        matches = list(NUMBERED_SECTION_PATTERN.finditer(text))
        if not matches:
            return []
        
        sections = []
        
        # Texte avant la premi√®re section
        if matches[0].start() > 0:
            preamble = text[:matches[0].start()].strip()
            if preamble:
                sections.append(TextSection(
                    title="Introduction",
                    content=preamble,
                    level=0,
                    start_pos=0
                ))
        
        for i, match in enumerate(matches):
            num = match.group(1)
            level = num.count('.')  # 1.1 = level 1, 1.1.1 = level 2
            start = match.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            
            # Titre = premi√®re ligne
            first_line_end = text.find('\n', start)
            if first_line_end == -1 or first_line_end > end:
                first_line_end = end
            title = text[start:first_line_end].strip()
            
            content = text[start:end].strip()
            
            sections.append(TextSection(
                title=title,
                content=content,
                level=level,
                article_number=num,
                start_pos=start
            ))
        
        return sections
    
    def _detect_uppercase_titles(self, text: str) -> List[TextSection]:
        """D√©tecte les titres en majuscules."""
        matches = list(UPPERCASE_TITLE_PATTERN.finditer(text))
        if not matches:
            return []
        
        sections = []
        
        # Texte avant le premier titre
        if matches[0].start() > 0:
            preamble = text[:matches[0].start()].strip()
            if preamble:
                sections.append(TextSection(
                    title="Introduction",
                    content=preamble,
                    level=0,
                    start_pos=0
                ))
        
        for i, match in enumerate(matches):
            title = match.group(1).strip()
            start = match.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            content = text[start:end].strip()
            
            sections.append(TextSection(
                title=title,
                content=content,
                level=0,
                start_pos=start
            ))
        
        return sections
    
    def _detect_paragraphs(self, text: str) -> List[TextSection]:
        """Fallback : d√©coupe par double saut de ligne."""
        paragraphs = re.split(r'\n\s*\n', text)
        sections = []
        
        for i, para in enumerate(paragraphs):
            para = para.strip()
            if not para:
                continue
            
            # Utiliser la premi√®re ligne comme titre (tronqu√©e)
            first_line = para.split('\n')[0][:80]
            
            sections.append(TextSection(
                title=first_line,
                content=para,
                level=0,
                start_pos=0  # Approximatif
            ))
        
        return sections
    
    # =========================================================================
    # PASSE 2 : Sections ‚Üí Groupes de phrases
    # =========================================================================
    
    def _sections_to_sentence_groups(self, sections: List[TextSection]) -> List[SentenceGroup]:
        """
        Convertit les sections en groupes de phrases avec m√©tadonn√©es.
        
        Chaque section est d√©coup√©e en phrases individuelles.
        Les phrases sont regroup√©es avec les m√©tadonn√©es de leur section.
        """
        groups = []
        heading_stack: List[str] = []  # Pile de titres pour la hi√©rarchie
        
        for section in sections:
            # Maintenir la hi√©rarchie des titres
            # On enl√®ve les titres de m√™me niveau ou sup√©rieur
            while len(heading_stack) > section.level:
                heading_stack.pop()
            heading_stack.append(section.title)
            
            # D√©couper le contenu en phrases
            sentences = self._split_into_sentences(section.content)
            
            if sentences:
                groups.append(SentenceGroup(
                    sentences=sentences,
                    section_title=section.title,
                    article_number=section.article_number,
                    heading_hierarchy=list(heading_stack)
                ))
        
        return groups
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        D√©coupe un texte en phrases.
        
        Respecte les fronti√®res naturelles :
        - Points, points d'exclamation, points d'interrogation
        - Ne coupe pas sur les abr√©viations courantes (M., etc.)
        - Ne coupe pas sur les num√©ros (art. 23.2)
        - Garde les listes √† puces comme phrases individuelles
        """
        if not text.strip():
            return []
        
        # D'abord, s√©parer les √©l√©ments de liste (tirets, puces, num√©rotation)
        lines = text.split('\n')
        sentences = []
        current_sentence = []
        
        for line in lines:
            line = line.strip()
            if not line:
                # Ligne vide = fin de phrase en cours
                if current_sentence:
                    sentences.append(' '.join(current_sentence))
                    current_sentence = []
                continue
            
            # D√©tecter les √©l√©ments de liste (tiret, puce, num√©ro suivi de point/parenth√®se)
            is_list_item = bool(re.match(r'^[-‚Ä¢‚óè‚ñ™]\s+', line)) or \
                           bool(re.match(r'^\d+[.)]\s+', line)) or \
                           bool(re.match(r'^[a-z][.)]\s+', line))
            
            if is_list_item:
                # Sauver la phrase en cours
                if current_sentence:
                    sentences.append(' '.join(current_sentence))
                    current_sentence = []
                # L'√©l√©ment de liste est une phrase √† part enti√®re
                sentences.append(line)
            else:
                # Ajouter √† la phrase en cours
                current_sentence.append(line)
                
                # Si la ligne se termine par un point/!/?
                if re.search(r'[.!?]\s*$', line):
                    sentences.append(' '.join(current_sentence))
                    current_sentence = []
        
        # Derni√®re phrase en cours
        if current_sentence:
            sentences.append(' '.join(current_sentence))
        
        # Deuxi√®me passe : re-d√©couper les phrases trop longues
        final_sentences = []
        for sent in sentences:
            if len(sent) > 1500:  # ~375 tokens, trop long pour une phrase
                # D√©couper sur les points internes
                sub_sentences = SENTENCE_ENDINGS.split(sent)
                final_sentences.extend(sub_sentences)
            else:
                final_sentences.append(sent)
        
        # Filtrer les phrases vides
        return [s.strip() for s in final_sentences if s.strip()]
    
    # =========================================================================
    # PASSE 3 : Groupes de phrases ‚Üí Chunks avec overlap
    # =========================================================================
    
    def _merge_into_chunks(
        self, 
        groups: List[SentenceGroup]
    ) -> List[Tuple[SentenceGroup, str]]:
        """
        Regroupe les phrases en chunks de taille cible avec overlap.
        
        Algorithme :
        1. Pour chaque groupe de phrases (section), on accumule les phrases
        2. Quand on atteint chunk_size tokens, on finalise le chunk
        3. On garde les derni√®res phrases (overlap) comme d√©but du chunk suivant
        4. Si une section enti√®re tient dans un chunk, on la garde intacte
        
        Returns:
            Liste de (SentenceGroup metadata, texte du chunk)
        """
        if not groups:
            return []
        
        chunks: List[Tuple[SentenceGroup, str]] = []
        
        for group in groups:
            # Si le groupe entier tient dans un chunk, on le garde tel quel
            group_tokens = group.token_estimate
            
            if group_tokens <= self._chunk_size:
                # Section enti√®re = un chunk (pr√©serve l'unit√© s√©mantique)
                # Ajouter le titre comme contexte
                chunk_text = self._format_chunk_with_context(group)
                chunks.append((group, chunk_text))
            else:
                # Section trop longue ‚Üí sous-d√©couper avec overlap
                sub_chunks = self._split_group_with_overlap(group)
                chunks.extend(sub_chunks)
        
        return chunks
    
    def _split_group_with_overlap(
        self, 
        group: SentenceGroup
    ) -> List[Tuple[SentenceGroup, str]]:
        """
        Sous-d√©coupe un groupe de phrases trop long en chunks avec overlap.
        
        L'overlap se fait au niveau des phrases : on reprend les derni√®res
        phrases du chunk pr√©c√©dent comme d√©but du chunk suivant.
        """
        chunks = []
        sentences = group.sentences
        
        if not sentences:
            return []
        
        current_sentences: List[str] = []
        current_tokens = 0
        
        # Pr√©fixe contextuel (titre de section/article)
        context_prefix = ""
        if group.article_number:
            context_prefix = f"[Article {group.article_number}] "
        elif group.section_title:
            context_prefix = f"[{group.section_title[:60]}] "
        prefix_tokens = len(context_prefix) // 4
        
        i = 0
        while i < len(sentences):
            sent = sentences[i]
            sent_tokens = len(sent) // 4
            
            # Si une phrase unique d√©passe chunk_size, on la prend quand m√™me
            if not current_sentences and sent_tokens > self._chunk_size - prefix_tokens:
                sub_group = SentenceGroup(
                    sentences=[sent],
                    section_title=group.section_title,
                    article_number=group.article_number,
                    heading_hierarchy=group.heading_hierarchy
                )
                chunk_text = context_prefix + sent
                chunks.append((sub_group, chunk_text))
                i += 1
                continue
            
            # Ajouter la phrase si elle tient
            if current_tokens + sent_tokens + prefix_tokens <= self._chunk_size:
                current_sentences.append(sent)
                current_tokens += sent_tokens
                i += 1
            else:
                # Finaliser le chunk courant
                if current_sentences:
                    sub_group = SentenceGroup(
                        sentences=list(current_sentences),
                        section_title=group.section_title,
                        article_number=group.article_number,
                        heading_hierarchy=group.heading_hierarchy
                    )
                    chunk_text = context_prefix + " ".join(current_sentences)
                    chunks.append((sub_group, chunk_text))
                    
                    # Overlap : reprendre les derni√®res phrases
                    overlap_sentences = self._compute_overlap(current_sentences)
                    overlap_tokens = sum(len(s) // 4 for s in overlap_sentences)
                    
                    # PROTECTION BOUCLE INFINIE : si l'overlap + prochaine phrase
                    # d√©passe la taille cible, on FORCE l'avancement en vidant l'overlap
                    if overlap_tokens + sent_tokens + prefix_tokens > self._chunk_size:
                        # La phrase est trop grosse m√™me avec juste l'overlap ‚Üí on prend
                        # la phrase seule dans le prochain chunk (sans overlap)
                        current_sentences = []
                        current_tokens = 0
                    else:
                        current_sentences = overlap_sentences
                        current_tokens = overlap_tokens
                else:
                    i += 1  # √âviter boucle infinie
        
        # Dernier chunk
        if current_sentences:
            sub_group = SentenceGroup(
                sentences=list(current_sentences),
                section_title=group.section_title,
                article_number=group.article_number,
                heading_hierarchy=group.heading_hierarchy
            )
            chunk_text = context_prefix + " ".join(current_sentences)
            chunks.append((sub_group, chunk_text))
        
        return chunks
    
    def _compute_overlap(self, sentences: List[str]) -> List[str]:
        """
        Calcule les phrases d'overlap (derni√®res phrases du chunk pr√©c√©dent).
        
        Prend les derni√®res phrases jusqu'√† atteindre chunk_overlap tokens.
        Ne coupe jamais une phrase.
        """
        if not sentences or self._chunk_overlap <= 0:
            return []
        
        overlap = []
        overlap_tokens = 0
        
        for sent in reversed(sentences):
            sent_tokens = len(sent) // 4
            if overlap_tokens + sent_tokens > self._chunk_overlap:
                break
            overlap.insert(0, sent)
            overlap_tokens += sent_tokens
        
        return overlap
    
    def _format_chunk_with_context(self, group: SentenceGroup) -> str:
        """
        Formate un chunk avec son contexte hi√©rarchique en pr√©fixe.
        
        Ex: "[Article 23.2 - R√©versibilit√©] Le prestataire s'engage..."
        """
        prefix_parts = []
        
        if group.article_number:
            prefix_parts.append(f"Article {group.article_number}")
        
        if group.section_title and group.section_title != f"Article {group.article_number}":
            # √âviter la redondance si le titre est juste "Article X"
            clean_title = group.section_title
            # Tronquer les titres trop longs
            if len(clean_title) > 80:
                clean_title = clean_title[:77] + "..."
            prefix_parts.append(clean_title)
        
        prefix = ""
        if prefix_parts:
            prefix = "[" + " - ".join(prefix_parts) + "] "
        
        return prefix + " ".join(group.sentences)


# Singleton pour usage global
_chunker: Optional[SemanticChunker] = None


def get_chunker() -> SemanticChunker:
    """Retourne l'instance singleton du SemanticChunker."""
    global _chunker
    if _chunker is None:
        _chunker = SemanticChunker()
    return _chunker
