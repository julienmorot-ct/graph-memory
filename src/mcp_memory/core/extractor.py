# -*- coding: utf-8 -*-
"""
ExtractorService - Extraction d'entit√©s et relations via LLMaaS.

Utilise l'API LLMaaS Cloud Temple (compatible OpenAI) pour extraire
les entit√©s, relations et concepts √† partir de texte.
"""

import sys
import json
from typing import Optional, List
from tenacity import retry, stop_after_attempt, wait_exponential

from openai import AsyncOpenAI
from openai import APIError, APITimeoutError

from ..config import get_settings
from .models import (
    ExtractionResult, ExtractedEntity, ExtractedRelation,
    EntityType, RelationType
)
from .ontology import Ontology, get_ontology_manager


# Prompt d'extraction MINIMAL (fallback sans ontologie).
# Toute la logique m√©tier (types d'entit√©s, relations, r√®gles) vient de l'ontologie.
# Ce prompt n'est utilis√© que par extract_from_text() quand aucune ontologie n'est charg√©e.
EXTRACTION_PROMPT = """Tu es un expert en extraction d'information structur√©e. Analyse le document suivant et extrait les entit√©s et relations importantes.

DOCUMENT:
---
{document_text}
---

INSTRUCTIONS:
1. Identifie les entit√©s nomm√©es (personnes, organisations, lieux, concepts, valeurs)
2. Identifie les relations entre ces entit√©s
3. Fournis un bref r√©sum√©

Les noms d'entit√©s doivent √™tre explicites et inclure les valeurs quand pertinent.
Cr√©e des relations ENTRE les entit√©s les plus sp√©cifiques, pas tout vers une entit√© centrale.
Utilise des types de relations descriptifs (SIGNED_BY, HAS_DURATION, DEFINES, etc.) plut√¥t que RELATED_TO.

R√©ponds UNIQUEMENT avec un JSON valide:
```json
{{
  "entities": [
    {{"name": "Nom de l'entit√©", "type": "Person|Organization|Concept|Other", "description": "Description courte"}}
  ],
  "relations": [
    {{"from_entity": "Nom entit√© source", "to_entity": "Nom entit√© cible", "type": "TYPE_RELATION", "description": "Description"}}
  ],
  "summary": "R√©sum√© du document en 2-3 phrases",
  "key_topics": ["sujet1", "sujet2"]
}}
```
"""


class ExtractorService:
    """
    Service d'extraction via LLMaaS.
    
    Utilise le mod√®le gpt-oss:120b de Cloud Temple pour extraire
    les entit√©s et relations structur√©es depuis un texte.
    """
    
    def __init__(self):
        """Initialise le client OpenAI compatible."""
        settings = get_settings()
        
        self._client = AsyncOpenAI(
            base_url=settings.llmaas_base_url,
            api_key=settings.llmaas_api_key,
            timeout=settings.extraction_timeout_seconds
        )
        self._model = settings.llmaas_model
        self._max_tokens = settings.llmaas_max_tokens
        self._temperature = settings.llmaas_temperature
        self._max_text_length = settings.extraction_max_text_length
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    async def extract_from_text(
        self,
        text: str,
    ) -> ExtractionResult:
        """
        Extrait les entit√©s et relations d'un texte.
        
        Args:
            text: Texte √† analyser
            
        Returns:
            ExtractionResult avec entit√©s, relations, r√©sum√©
        """
        # Tronquer si n√©cessaire (limite depuis config EXTRACTION_MAX_TEXT_LENGTH)
        if len(text) > self._max_text_length:
            text = text[:self._max_text_length] + "\n\n[Document tronqu√©...]"
        
        prompt = EXTRACTION_PROMPT.format(document_text=text)
        
        try:
            print(f"üîç [Extractor] Extraction en cours ({len(text)} chars)...", file=sys.stderr)
            
            response = await self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un assistant sp√©cialis√© dans l'extraction d'information structur√©e. Tu r√©ponds uniquement en JSON valide."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=self._max_tokens,
                temperature=self._temperature
                # Note: response_format non support√© par LLMaaS Cloud Temple
            )
            
            # Parser la r√©ponse - DEBUG COMPLET
            print(f"üîç [Extractor] DEBUG response type: {type(response)}", file=sys.stderr)
            print(f"üîç [Extractor] DEBUG choices count: {len(response.choices)}", file=sys.stderr)
            if response.choices:
                print(f"üîç [Extractor] DEBUG message: {response.choices[0].message}", file=sys.stderr)
                print(f"üîç [Extractor] DEBUG finish_reason: {response.choices[0].finish_reason}", file=sys.stderr)
            
            content = response.choices[0].message.content
            if content is None:
                print(f"‚ö†Ô∏è [Extractor] R√©ponse LLM vide - message complet: {response.choices[0].message}", file=sys.stderr)
                return ExtractionResult(summary=None)
            
            print(f"üîç [Extractor] DEBUG content length: {len(content)}", file=sys.stderr)
            result = self._parse_extraction(content)
            
            print(f"‚úÖ [Extractor] Extrait: {len(result.entities)} entit√©s, {len(result.relations)} relations", file=sys.stderr)
            
            return result
            
        except APITimeoutError:
            print(f"‚è∞ [Extractor] Timeout - le document est peut-√™tre trop long", file=sys.stderr)
            raise
        except APIError as e:
            print(f"‚ùå [Extractor] Erreur API: {e}", file=sys.stderr)
            raise
    
    def _parse_extraction(self, content: str, known_relation_types: Optional[set] = None) -> ExtractionResult:
        """
        Parse la r√©ponse JSON du LLM.
        
        Args:
            content: Contenu JSON brut du LLM
            known_relation_types: Types de relations connus (depuis l'ontologie).
                                   Si None, utilise BASE_RELATION_TYPES.
        """
        try:
            # Nettoyer le contenu (parfois le LLM ajoute des ```json)
            content = content.strip()
            if content.startswith("```"):
                # Trouver le premier { et le dernier }
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]
            
            data = json.loads(content)
            
            # Parser les entit√©s
            entities = []
            for e in data.get("entities", []):
                entity_type = self._parse_entity_type(e.get("type", "Other"))
                entities.append(ExtractedEntity(
                    name=e.get("name", "").strip(),
                    type=entity_type,
                    description=e.get("description")
                ))
            
            # Parser les relations ‚Äî avec les types connus de l'ontologie
            relations = []
            for r in data.get("relations", []):
                rel_type = self._parse_relation_type(
                    r.get("type", "RELATED_TO"),
                    known_types=known_relation_types
                )
                relations.append(ExtractedRelation(
                    from_entity=r.get("from_entity", "").strip(),
                    to_entity=r.get("to_entity", "").strip(),
                    type=rel_type,
                    description=r.get("description")
                ))
            
            return ExtractionResult(
                entities=entities,
                relations=relations,
                summary=data.get("summary"),
                key_topics=data.get("key_topics", [])
            )
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è [Extractor] Erreur parsing JSON: {e}", file=sys.stderr)
            print(f"   Contenu re√ßu: {content[:200]}...", file=sys.stderr)
            # Retourner un r√©sultat vide plut√¥t que crasher
            return ExtractionResult(summary=None)
    
    @staticmethod
    def _parse_entity_type(type_str: str) -> EntityType:
        """Convertit une string en EntityType."""
        type_map = {
            "person": EntityType.PERSON,
            "organization": EntityType.ORGANIZATION,
            "concept": EntityType.CONCEPT,
            "location": EntityType.LOCATION,
            "date": EntityType.DATE,
            "product": EntityType.PRODUCT,
            "service": EntityType.SERVICE,
            "clause": EntityType.CLAUSE,
            "certification": EntityType.CERTIFICATION,
            "metric": EntityType.METRIC,
            "duration": EntityType.DURATION,
            "amount": EntityType.AMOUNT,
        }
        return type_map.get(type_str.lower(), EntityType.OTHER)
    
    # Types de base (utilis√©s quand aucune ontologie n'est charg√©e)
    BASE_RELATION_TYPES = {
        "MENTIONS", "DEFINES", "RELATED_TO", "BELONGS_TO",
        "SIGNED_BY", "CREATED_BY", "REFERENCES", "CONTAINS",
        "HAS_VALUE", "CERTIFIES", "PART_OF",
    }
    
    @staticmethod
    def _parse_relation_type(type_str: str, known_types: Optional[set] = None) -> str:
        """
        Convertit une string en type de relation.
        
        Accepte les types d√©finis par l'ontologie (dynamique).
        Les types inconnus qui ont un format valide (MAJ + underscores) sont accept√©s tels quels.
        
        Args:
            type_str: Type brut retourn√© par le LLM
            known_types: Set de types connus (provenant de l'ontologie). Si None, utilise BASE_RELATION_TYPES.
        """
        # Normaliser : majuscules, underscores
        normalized = type_str.strip().upper().replace(" ", "_").replace("-", "_")
        
        # Types connus depuis l'ontologie (ou base par d√©faut)
        valid_types = known_types or ExtractorService.BASE_RELATION_TYPES
        
        if normalized in valid_types:
            return normalized
        
        # Accepter tout type au format valide (MAJ + underscores) ‚Äî le LLM peut inventer
        if normalized.replace("_", "").isalpha() and normalized == normalized.upper():
            return normalized
        
        return "RELATED_TO"
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    async def extract_with_ontology(
        self,
        text: str,
        ontology_name: str = "default",
    ) -> ExtractionResult:
        """
        Extrait les entit√©s et relations d'un texte en utilisant une ontologie.
        
        Args:
            text: Texte √† analyser
            ontology_name: Nom de l'ontologie √† utiliser (ex: "legal", "cloud")
            
        Returns:
            ExtractionResult avec entit√©s, relations, r√©sum√©
        """
        # Charger l'ontologie ‚Äî OBLIGATOIRE
        ontology_manager = get_ontology_manager()
        ontology = ontology_manager.get_ontology(ontology_name)
        
        if not ontology:
            available = [o["name"] for o in ontology_manager.list_ontologies()]
            raise ValueError(
                f"Ontologie '{ontology_name}' introuvable. "
                f"Ontologies disponibles: {available}. "
                f"Chaque m√©moire DOIT avoir une ontologie valide."
            )
        
        # Tronquer si n√©cessaire (limite depuis config EXTRACTION_MAX_TEXT_LENGTH)
        if len(text) > self._max_text_length:
            text = text[:self._max_text_length] + "\n\n[Document tronqu√©...]"
        
        # Construire le prompt avec l'ontologie
        prompt = ontology.build_prompt(text)
        
        try:
            print(f"üîç [Extractor] Extraction avec ontologie '{ontology.name}' ({len(text)} chars)...", file=sys.stderr)
            
            response = await self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un assistant sp√©cialis√© dans l'extraction d'information structur√©e. Tu r√©ponds uniquement en JSON valide."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=self._max_tokens,
                temperature=self._temperature
            )
            
            content = response.choices[0].message.content
            if content is None:
                print(f"‚ö†Ô∏è [Extractor] R√©ponse LLM vide", file=sys.stderr)
                return ExtractionResult(summary=None)
            
            # Extraire les types de relations depuis l'ontologie charg√©e
            ontology_relation_types = {
                rt.name.upper() for rt in ontology.relation_types
            } | self.BASE_RELATION_TYPES  # Union avec les types de base
            
            print(f"üîó [Extractor] Types de relations ontologie '{ontology.name}': {sorted(ontology_relation_types)}", file=sys.stderr)
            
            result = self._parse_extraction(content, known_relation_types=ontology_relation_types)
            
            print(f"‚úÖ [Extractor] Extrait ({ontology.name}): {len(result.entities)} entit√©s, {len(result.relations)} relations", file=sys.stderr)
            
            return result
            
        except APITimeoutError:
            print(f"‚è∞ [Extractor] Timeout - le document est peut-√™tre trop long", file=sys.stderr)
            raise
        except APIError as e:
            print(f"‚ùå [Extractor] Erreur API: {e}", file=sys.stderr)
            raise

    # =========================================================================
    # Extraction chunked (gros documents)
    # =========================================================================

    async def extract_with_ontology_chunked(
        self,
        text: str,
        ontology_name: str = "default",
        progress_callback=None,
    ) -> ExtractionResult:
        """
        Extrait les entit√©s et relations d'un texte long en le d√©coupant en chunks.
        
        Strat√©gie s√©quentielle avec contexte cumulatif :
        - Si le texte est court (< extraction_chunk_size), d√©l√®gue √† extract_with_ontology()
        - Sinon, d√©coupe en chunks aux fronti√®res de sections
        - Chaque chunk re√ßoit le contexte des entit√©s/relations d√©j√† extraites
        - Les r√©sultats sont fusionn√©s √† la fin
        
        Args:
            text: Texte complet du document
            ontology_name: Nom de l'ontologie √† utiliser
            
        Returns:
            ExtractionResult fusionn√© avec toutes les entit√©s et relations
        """
        settings = get_settings()
        chunk_size = settings.extraction_chunk_size
        
        # Si le texte tient dans un seul chunk, pas besoin de d√©couper
        if len(text) <= chunk_size:
            print(f"üìÑ [Extractor] Document court ({len(text)} chars ‚â§ {chunk_size}) ‚Üí extraction simple",
                  file=sys.stderr)
            if progress_callback:
                await progress_callback("extraction_start", {
                    "chunks_total": 1, "chunk_current": 1,
                    "text_length": len(text), "mode": "single"
                })
            result = await self.extract_with_ontology(text, ontology_name)
            if progress_callback:
                await progress_callback("extraction_chunk_done", {
                    "chunk": 1, "chunks_total": 1,
                    "entities_new": len(result.entities),
                    "relations_new": len(result.relations),
                    "entities_cumul": len(result.entities),
                    "relations_cumul": len(result.relations),
                })
            return result
        
        # D√©couper le texte en chunks aux fronti√®res de sections
        chunks = self._split_text_for_extraction(text, chunk_size)
        print(f"üìê [Extractor] Document long ({len(text)} chars) ‚Üí {len(chunks)} chunks d'extraction",
              file=sys.stderr)
        
        # Notifier le d√©but de l'extraction multi-chunk
        if progress_callback:
            await progress_callback("extraction_start", {
                "chunks_total": len(chunks), "chunk_current": 0,
                "text_length": len(text), "mode": "chunked",
                "chunk_sizes": [len(c) for c in chunks],
            })
        
        # Charger l'ontologie (une seule fois)
        ontology_manager = get_ontology_manager()
        ontology = ontology_manager.get_ontology(ontology_name)
        if not ontology:
            available = [o["name"] for o in ontology_manager.list_ontologies()]
            raise ValueError(
                f"Ontologie '{ontology_name}' introuvable. "
                f"Ontologies disponibles: {available}."
            )
        
        # Types de relations connus depuis l'ontologie
        ontology_relation_types = {
            rt.name.upper() for rt in ontology.relation_types
        } | self.BASE_RELATION_TYPES
        
        # Extraction s√©quentielle avec contexte cumulatif
        all_entities: List[ExtractedEntity] = []
        all_relations: List[ExtractedRelation] = []
        all_summaries: List[str] = []
        all_key_topics: List[str] = []
        
        for i, chunk_text in enumerate(chunks):
            chunk_num = i + 1
            
            # Construire le contexte cumulatif (vide pour le premier chunk)
            cumulative_context = ""
            if all_entities or all_relations:
                cumulative_context = self._build_cumulative_context(all_entities, all_relations)
            
            print(f"üîÑ [Extractor] Chunk {chunk_num}/{len(chunks)} "
                  f"({len(chunk_text)} chars, contexte cumulatif: {len(all_entities)} entit√©s, "
                  f"{len(all_relations)} relations)", file=sys.stderr)
            
            # Construire le prompt avec contexte cumulatif
            prompt = ontology.build_prompt(chunk_text, cumulative_context=cumulative_context)
            
            try:
                response = await self._client.chat.completions.create(
                    model=self._model,
                    messages=[
                        {
                            "role": "system",
                            "content": "Tu es un assistant sp√©cialis√© dans l'extraction d'information structur√©e. Tu r√©ponds uniquement en JSON valide."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=self._max_tokens,
                    temperature=self._temperature
                )
                
                content = response.choices[0].message.content
                if content is None:
                    print(f"‚ö†Ô∏è [Extractor] Chunk {chunk_num}: r√©ponse LLM vide", file=sys.stderr)
                    continue
                
                result = self._parse_extraction(content, known_relation_types=ontology_relation_types)
                
                print(f"‚úÖ [Extractor] Chunk {chunk_num}: +{len(result.entities)} entit√©s, "
                      f"+{len(result.relations)} relations", file=sys.stderr)
                
                # Accumuler les r√©sultats
                all_entities.extend(result.entities)
                all_relations.extend(result.relations)
                if result.summary:
                    all_summaries.append(result.summary)
                all_key_topics.extend(result.key_topics)
                
                # Notifier la progression
                if progress_callback:
                    await progress_callback("extraction_chunk_done", {
                        "chunk": chunk_num, "chunks_total": len(chunks),
                        "chunk_chars": len(chunk_text),
                        "entities_new": len(result.entities),
                        "relations_new": len(result.relations),
                        "entities_cumul": len(all_entities),
                        "relations_cumul": len(all_relations),
                    })
                
            except APITimeoutError:
                print(f"‚è∞ [Extractor] Timeout chunk {chunk_num}/{len(chunks)} ‚Äî on continue", file=sys.stderr)
                # On continue avec les chunks suivants au lieu de tout perdre
                continue
            except APIError as e:
                print(f"‚ùå [Extractor] Erreur API chunk {chunk_num}/{len(chunks)}: {e}", file=sys.stderr)
                raise
        
        # Fusionner les r√©sultats
        merged = self._merge_extraction_results(all_entities, all_relations, all_summaries, all_key_topics)
        
        print(f"üèÅ [Extractor] Extraction chunked termin√©e: "
              f"{len(merged.entities)} entit√©s, {len(merged.relations)} relations "
              f"(depuis {len(chunks)} chunks)", file=sys.stderr)
        
        return merged

    def _split_text_for_extraction(self, text: str, chunk_size: int) -> List[str]:
        """
        D√©coupe un texte long en chunks pour l'extraction graph.
        
        Strat√©gie : d√©coupe aux fronti√®res de sections (double saut de ligne,
        articles, titres) pour ne jamais couper au milieu d'un paragraphe.
        
        Args:
            text: Texte complet du document
            chunk_size: Taille max en caract√®res par chunk
            
        Returns:
            Liste de chunks de texte
        """
        import re
        
        # Identifier les points de coupe naturels (double saut de ligne)
        # On pr√©f√®re couper aux fronti√®res de sections/articles
        sections = re.split(r'(\n\s*\n)', text)
        
        chunks = []
        current_chunk = ""
        
        for section in sections:
            # Si ajouter cette section d√©passe la taille ET qu'on a d√©j√† du contenu
            if len(current_chunk) + len(section) > chunk_size and current_chunk.strip():
                chunks.append(current_chunk.strip())
                current_chunk = section
            else:
                current_chunk += section
        
        # Dernier chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # Si un chunk est encore trop gros (section unique tr√®s longue),
        # on le re-d√©coupe sur les simples sauts de ligne
        final_chunks = []
        for chunk in chunks:
            if len(chunk) > chunk_size * 1.5:  # Tol√©rance de 50%
                sub_chunks = self._force_split_chunk(chunk, chunk_size)
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append(chunk)
        
        return final_chunks

    def _force_split_chunk(self, text: str, chunk_size: int) -> List[str]:
        """
        D√©coupe forc√©e d'un chunk trop gros (section unique tr√®s longue).
        
        Coupe aux fronti√®res de lignes pour ne jamais couper mid-phrase.
        """
        lines = text.split('\n')
        chunks = []
        current_chunk = ""
        
        for line in lines:
            if len(current_chunk) + len(line) + 1 > chunk_size and current_chunk.strip():
                chunks.append(current_chunk.strip())
                current_chunk = line + '\n'
            else:
                current_chunk += line + '\n'
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    @staticmethod
    def _build_cumulative_context(
        entities: List[ExtractedEntity],
        relations: List[ExtractedRelation]
    ) -> str:
        """
        Construit un r√©sum√© compact des entit√©s et relations d√©j√† extraites.
        
        Format optimis√© pour le budget tokens :
        - ~10-15 tokens par entit√©
        - ~15-20 tokens par relation
        - Total typique : 2-3K tokens pour 100 entit√©s + 100 relations
        
        Args:
            entities: Entit√©s d√©j√† extraites
            relations: Relations d√©j√† extraites
            
        Returns:
            Texte compact du contexte cumulatif
        """
        parts = []
        
        # Liste compacte des entit√©s (nom + type)
        if entities:
            entity_lines = []
            for e in entities:
                type_str = e.type.value if hasattr(e.type, 'value') else str(e.type)
                entity_lines.append(f"- {e.name} ({type_str})")
            parts.append("ENTIT√âS D√âJ√Ä EXTRAITES:\n" + "\n".join(entity_lines))
        
        # Liste compacte des relations (from --TYPE--> to)
        if relations:
            relation_lines = []
            for r in relations:
                relation_lines.append(f"- {r.from_entity} --{r.type}--> {r.to_entity}")
            parts.append("RELATIONS D√âJ√Ä EXTRAITES:\n" + "\n".join(relation_lines))
        
        return "\n\n".join(parts)

    @staticmethod
    def _merge_extraction_results(
        all_entities: List[ExtractedEntity],
        all_relations: List[ExtractedRelation],
        all_summaries: List[str],
        all_key_topics: List[str]
    ) -> ExtractionResult:
        """
        Fusionne les r√©sultats de N extractions chunked.
        
        D√©duplication :
        - Entit√©s : par (nom normalis√©, type), on garde la description la plus longue
        - Relations : par (from, to, type), on garde la description la plus longue
        - Key topics : unicit√©
        - Summaries : concat√©nation
        
        Args:
            all_entities: Toutes les entit√©s extraites (avec doublons potentiels)
            all_relations: Toutes les relations extraites
            all_summaries: R√©sum√©s partiels de chaque chunk
            all_key_topics: Topics de chaque chunk
            
        Returns:
            ExtractionResult fusionn√© et d√©dupliqu√©
        """
        # D√©dupliquer les entit√©s par (nom normalis√©, type)
        entity_map = {}  # (name_lower, type) -> ExtractedEntity
        for e in all_entities:
            key = (e.name.strip().lower(), e.type)
            if key not in entity_map:
                entity_map[key] = e
            else:
                # Garder la description la plus longue (la plus riche)
                existing = entity_map[key]
                if e.description and (not existing.description or len(e.description) > len(existing.description)):
                    entity_map[key] = ExtractedEntity(
                        name=existing.name,  # Garder le nom original (premi√®re occurrence)
                        type=existing.type,
                        description=e.description
                    )
        
        # D√©dupliquer les relations par (from_lower, to_lower, type)
        relation_map = {}  # (from, to, type) -> ExtractedRelation
        for r in all_relations:
            key = (r.from_entity.strip().lower(), r.to_entity.strip().lower(), r.type)
            if key not in relation_map:
                relation_map[key] = r
            else:
                existing = relation_map[key]
                if r.description and (not existing.description or len(r.description) > len(existing.description)):
                    relation_map[key] = ExtractedRelation(
                        from_entity=existing.from_entity,
                        to_entity=existing.to_entity,
                        type=existing.type,
                        description=r.description
                    )
        
        # Fusionner les r√©sum√©s
        merged_summary = " ".join(all_summaries) if all_summaries else None
        
        # D√©dupliquer les topics
        seen_topics = set()
        unique_topics = []
        for topic in all_key_topics:
            topic_lower = topic.strip().lower()
            if topic_lower not in seen_topics:
                seen_topics.add(topic_lower)
                unique_topics.append(topic.strip())
        
        return ExtractionResult(
            entities=list(entity_map.values()),
            relations=list(relation_map.values()),
            summary=merged_summary,
            key_topics=unique_topics
        )

    async def test_connection(self) -> dict:
        """Teste la connexion au LLMaaS."""
        try:
            response = await self._client.chat.completions.create(
                model=self._model,
                messages=[{"role": "user", "content": "R√©ponds juste 'OK'"}],
                max_tokens=10
            )
            
            return {
                "status": "ok",
                "model": self._model,
                "message": "Connexion LLMaaS r√©ussie"
            }
            
        except APIError as e:
            return {
                "status": "error",
                "model": self._model,
                "message": f"Erreur LLMaaS: {str(e)}"
            }


    async def generate_answer(self, prompt: str) -> str:
        """
        G√©n√®re une r√©ponse √† partir d'un prompt.
        
        Utilis√© pour le Q&A sur le graphe de connaissances.
        
        Args:
            prompt: Prompt complet avec contexte et question
            
        Returns:
            R√©ponse g√©n√©r√©e par le LLM
        """
        try:
            response = await self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un assistant expert qui r√©pond √† des questions bas√©es sur un graphe de connaissances. R√©ponds de mani√®re concise et pr√©cise."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,  # Plus d√©terministe pour les r√©ponses factuelles
                max_tokens=self._max_tokens
            )
            
            return response.choices[0].message.content or "Pas de r√©ponse g√©n√©r√©e."
            
        except Exception as e:
            print(f"‚ùå [Extractor] Erreur g√©n√©ration r√©ponse: {e}", file=sys.stderr)
            return f"Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}"


# Singleton pour usage global
_extractor_service: Optional[ExtractorService] = None


def get_extractor_service() -> ExtractorService:
    """Retourne l'instance singleton du ExtractorService."""
    global _extractor_service
    if _extractor_service is None:
        _extractor_service = ExtractorService()
    return _extractor_service
