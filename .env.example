# =============================================================================
# MCP Memory Service — Configuration
# =============================================================================
# Copiez ce fichier vers .env et remplissez vos valeurs :
#   cp .env.example .env
#
# Toutes les variables sont lues par pydantic-settings (src/mcp_memory/config.py).
# Les valeurs par défaut sont indiquées entre parenthèses.
# =============================================================================

# =============================================================================
# S3 — Stockage objet (documents sources + ontologies)
# =============================================================================
# Endpoint S3 compatible (Cloud Temple, AWS, MinIO, Dell ECS…)
S3_ENDPOINT_URL=https://your-bucket.s3.fr1.cloud-temple.com

# Credentials S3
S3_ACCESS_KEY_ID=your_access_key_id
S3_SECRET_ACCESS_KEY=your_secret_access_key

# Nom du bucket
S3_BUCKET_NAME=your-bucket-name

# Région S3 (défaut: fr1)
# S3_REGION_NAME=fr1

# =============================================================================
# LLMaaS — Service d'IA (extraction d'entités + Q&A)
# =============================================================================
# URL de l'API compatible OpenAI
LLMAAS_API_URL=https://api.ai.cloud-temple.com

# Clé d'API
LLMAAS_API_KEY=your_llmaas_api_key

# Modèle à utiliser (défaut: gpt-oss:120b)
# Modèles testés : gpt-oss:120b, granite3.3:8b
LLMAAS_MODEL=gpt-oss:120b

# Nombre max de tokens par réponse (défaut: 60000)
# gpt-oss:120b utilise du chain-of-thought, nécessite beaucoup de tokens
# LLMAAS_MAX_TOKENS=60000

# Température (défaut: 1.0 — gpt-oss:120b requiert température 1.0)
# LLMAAS_TEMPERATURE=1.0

# Longueur max du texte total envoyé au LLM en caractères (défaut: 950000)
# Au-delà, le document est tronqué. Réduire si le modèle a une fenêtre de contexte petite.
# EXTRACTION_MAX_TEXT_LENGTH=950000

# Taille max d'un chunk d'extraction graph en caractères (défaut: 25000, ~6K tokens)
# Au-delà de cette taille, le document est découpé en N chunks extraits séquentiellement
# avec contexte cumulatif (les entités/relations des chunks précédents sont passées au suivant).
# 25000 chars ≈ 6K tokens, laisse ~54K tokens pour le prompt ontologie + réponse JSON.
# EXTRACTION_CHUNK_SIZE=25000

# =============================================================================
# Embedding — Vectorisation (pour RAG)
# =============================================================================
# Modèle d'embedding (via LLMaaS Cloud Temple)
# bge-m3:567m = multilingue, 1024 dimensions
LLMAAS_EMBEDDING_MODEL=bge-m3:567m

# Dimension des vecteurs (doit correspondre au modèle, défaut: 1024)
# LLMAAS_EMBEDDING_DIMENSIONS=1024

# =============================================================================
# Qdrant — Base de données vectorielle (RAG)
# =============================================================================
# URL de connexion (défaut: http://qdrant:6333 pour Docker)
QDRANT_URL=http://qdrant:6333

# Préfixe des collections (défaut: memory_)
# Chaque mémoire crée une collection memory_{safe_id}
# QDRANT_COLLECTION_PREFIX=memory_

# =============================================================================
# Chunking sémantique
# =============================================================================
# Taille cible par chunk en tokens (défaut: 500)
# CHUNK_SIZE=500

# Chevauchement entre chunks adjacents en tokens (défaut: 50)
# CHUNK_OVERLAP=50

# =============================================================================
# RAG — Recherche vectorielle
# =============================================================================
# Score cosinus minimum pour qu'un chunk soit retenu (0.0 à 1.0)
# En dessous de ce seuil, le chunk est considéré non pertinent et ignoré.
# Défaut: 0.65 (assez strict). Baisser à 0.5 pour être plus permissif.
# RAG_SCORE_THRESHOLD=0.58

# Nombre max de chunks retournés par Qdrant (défaut: 8)
# RAG_CHUNK_LIMIT=8

# =============================================================================
# Neo4j — Base de données de graphe
# =============================================================================
# URI de connexion (défaut: bolt://neo4j:7687 pour Docker)
NEO4J_URI=bolt://neo4j:7687

# Identifiants
NEO4J_USER=neo4j
NEO4J_PASSWORD=change_this_password

# Base de données (défaut: neo4j)
# NEO4J_DATABASE=neo4j

# =============================================================================
# WAF — Coraza Web Application Firewall (reverse proxy sécurisé)
# =============================================================================
# Port d'écoute du WAF en mode dev (seul port exposé à l'extérieur)
WAF_PORT=8080

# Adresse du site :
# - Dev  : ":8080"                              (HTTP simple, pas de TLS)
# - Prod : "graph-memory.votre-domaine.com"      (HTTPS automatique Let's Encrypt)
#
# En mode PROD avec nom de domaine :
#   1. Mettre le FQDN ici (ex: graph-memory.cloud-temple.com)
#   2. Dans docker-compose.yml, commenter le port WAF_PORT
#      et décommenter les ports 80 + 443
#   3. Caddy obtient et renouvelle automatiquement le certificat TLS
#   4. Les certificats sont stockés dans le volume waf_data
SITE_ADDRESS=:8080

# =============================================================================
# Serveur MCP (interne, derrière le WAF)
# =============================================================================
# Port d'écoute interne (défaut: 8002, accessible uniquement via le WAF)
MCP_SERVER_PORT=8002

# Adresse d'écoute (défaut: 0.0.0.0)
# MCP_SERVER_HOST=0.0.0.0

# Mode debug — logs détaillés (défaut: false)
MCP_SERVER_DEBUG=false

# Nom du serveur MCP (défaut: mcp-memory)
# MCP_SERVER_NAME=mcp-memory

# =============================================================================
# Authentification
# =============================================================================
# Clé bootstrap pour créer le premier token d'API.
# À changer impérativement en production !
ADMIN_BOOTSTRAP_KEY=change_me_in_production

# =============================================================================
# CLI — Variables pour piloter un serveur DISTANT
# =============================================================================
# Ces variables sont utilisées par la CLI (scripts/mcp_cli.py), PAS par le serveur.
# Elles sont prioritaires sur MCP_SERVER_URL et ADMIN_BOOTSTRAP_KEY.
#
# NE PAS les définir dans le .env du serveur ! Elles sont pour le poste CLIENT.
# Voir : scripts/README.md et DESIGN/DEPLOIEMENT_PRODUCTION.md §15
#
# MCP_URL=https://mcp-memory.example.com
# MCP_TOKEN=votre_bootstrap_key_production

# =============================================================================
# Limites et timeouts
# =============================================================================
# Taille max d'un document en Mo (défaut: 50)
# MAX_DOCUMENT_SIZE_MB=50

# Timeout extraction LLM en secondes (défaut: 600 = 10 min)
# Augmenté pour supporter les gros documents avec chain-of-thought (gpt-oss:120b)
# EXTRACTION_TIMEOUT_SECONDS=600

# Timeout upload S3 en secondes (défaut: 60)
# S3_UPLOAD_TIMEOUT_SECONDS=60

# Timeout requêtes Neo4j en secondes (défaut: 30)
# NEO4J_QUERY_TIMEOUT_SECONDS=30
